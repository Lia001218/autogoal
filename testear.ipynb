{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liazerquera/.pyenv/versions/3.9.16/lib/python3.9/site-packages/openml/datasets/functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "/Users/liazerquera/.pyenv/versions/3.9.16/lib/python3.9/site-packages/openml/datasets/functions.py:454: UserWarning: ``download_all_files`` is experimental and is likely to break with new releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta_Album_BRD_Mini\n"
     ]
    }
   ],
   "source": [
    "  # import openml\n",
    "import openml\n",
    "\n",
    "    # download dataset with DATASET_ID. Check Dataset detail page for DATASET_ID\n",
    "dataset = openml.datasets.get_dataset(\"44285\", download_data=True, download_all_files=True)\n",
    "\n",
    "    # display dataset info\n",
    "print(dataset.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/liazerquera/.openml/org/openml/www/datasets/44285/dataset.arff'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_path():\n",
    "    data_file_path = ''\n",
    "    data_name_path = ''\n",
    "    for i  in range(len(dataset.data_file) -1,0,-1):\n",
    "        if dataset.data_file[i] == '/':\n",
    "            data_file_path = dataset.data_file[:i+1]\n",
    "            break\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(dataset.name)):\n",
    "        if count == 2:\n",
    "          data_name_path = dataset.name[i:]\n",
    "          break\n",
    "        if dataset.name[i] == '_':\n",
    "            count += 1\n",
    "\n",
    "    return data_file_path + data_name_path + '/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y , _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = {}\n",
    "count = 0\n",
    "vectorized_y = []\n",
    "for i in range(y.shape[0]):\n",
    "    if y[i] not in ids:\n",
    "        ids[y[i]] = count\n",
    "        count += 1\n",
    "    vectorized_y.append(ids[y[i]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/liazerquera/.openml/org/openml/www/datasets/44285/dataset.arff'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "dataset.data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 44285\n",
    "dataset_name = 'BRD_Mini'\n",
    "ruta = build_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12600, 128, 128, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "X = []\n",
    "for i in os.listdir(ruta):\n",
    "    temp_path =os.path.join(ruta, i)\n",
    "    X.append(asarray(Image.open(temp_path)))\n",
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing autogoal-regex 0.2.0. Use pip install autogoal-regex to ensure all dependencies are installed correctly.\n",
      "Error importing autogoal-keras 0.2.0. Use pip install autogoal-keras to ensure all dependencies are installed correctly.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No pipelines can be found!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Lia-tesis/autogoal/autogoal/autogoal/kb/_algorithm.py:684\u001b[0m, in \u001b[0;36mbuild_pipeline_graph\u001b[0;34m(input_types, output_type, registry, max_list_depth)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 684\u001b[0m     reachable_from_end \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m(\n\u001b[1;32m    685\u001b[0m         nx\u001b[39m.\u001b[39;49mdfs_preorder_nodes(G\u001b[39m.\u001b[39;49mreverse(\u001b[39mFalse\u001b[39;49;00m), GraphSpace\u001b[39m.\u001b[39;49mEnd)\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m     unreachable_nodes \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(G\u001b[39m.\u001b[39mnodes) \u001b[39m-\u001b[39m reachable_from_end\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/networkx/algorithms/traversal/depth_first_search.py:345\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    344\u001b[0m edges \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mdfs_labeled_edges(G, source\u001b[39m=\u001b[39msource, depth_limit\u001b[39m=\u001b[39mdepth_limit)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mreturn\u001b[39;00m (v \u001b[39mfor\u001b[39;00m u, v, d \u001b[39min\u001b[39;00m edges \u001b[39mif\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/networkx/algorithms/traversal/depth_first_search.py:427\u001b[0m, in \u001b[0;36mdfs_labeled_edges\u001b[0;34m(G, source, depth_limit)\u001b[0m\n\u001b[1;32m    426\u001b[0m visited\u001b[39m.\u001b[39madd(start)\n\u001b[0;32m--> 427\u001b[0m stack \u001b[39m=\u001b[39m [(start, depth_limit, \u001b[39miter\u001b[39m(G[start]))]\n\u001b[1;32m    428\u001b[0m \u001b[39mwhile\u001b[39;00m stack:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/networkx/classes/graph.py:517\u001b[0m, in \u001b[0;36mGraph.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a dict of neighbors of node n.  Use: 'G[n]'.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mAtlasView({1: {}})\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madj[n]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/networkx/classes/coreviews.py:82\u001b[0m, in \u001b[0;36mAdjacencyView.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m AtlasView(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_atlas[name])\n",
      "\u001b[0;31mKeyError\u001b[0m: End()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/liazerquera/Desktop/Lia-tesis/autogoal/testear.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liazerquera/Desktop/Lia-tesis/autogoal/testear.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m classifier \u001b[39m=\u001b[39m AutoML(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liazerquera/Desktop/Lia-tesis/autogoal/testear.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dataset_type\u001b[39m=\u001b[39m ImageMetafeatureExtractor(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liazerquera/Desktop/Lia-tesis/autogoal/testear.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     search_algorithm\u001b[39m=\u001b[39m PESearch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liazerquera/Desktop/Lia-tesis/autogoal/testear.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     search_timeout\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m Hour,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liazerquera/Desktop/Lia-tesis/autogoal/testear.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liazerquera/Desktop/Lia-tesis/autogoal/testear.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(vectorized_y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liazerquera/Desktop/Lia-tesis/autogoal/testear.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(X, y, logger\u001b[39m=\u001b[39;49mRichLogger())   \n",
      "File \u001b[0;32m~/Desktop/Lia-tesis/autogoal/autogoal/autogoal/ml/_automl.py:148\u001b[0m, in \u001b[0;36mAutoML.fit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_type(y)\n\u001b[1;32m    147\u001b[0m search \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_algorithm(\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_pipeline_builder(),\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_fitness_fn(X, y),\n\u001b[1;32m    150\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state,\n\u001b[1;32m    151\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,\n\u001b[1;32m    152\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_kwargs,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_pipelines_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_scores_ \u001b[39m=\u001b[39m search\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_iterations,metafeature_instance\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_example\n\u001b[1;32m    157\u001b[0m     , \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_pipeline(X, y)\n",
      "File \u001b[0;32m~/Desktop/Lia-tesis/autogoal/autogoal/autogoal/ml/_automl.py:121\u001b[0m, in \u001b[0;36mAutoML.make_pipeline_builder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m         remote_registry \u001b[39m=\u001b[39m find_remote_classes(\n\u001b[1;32m    114\u001b[0m             sources\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremote_sources,\n\u001b[1;32m    115\u001b[0m             include\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minclude_filter,\n\u001b[1;32m    116\u001b[0m             exclude\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexclude_filter,\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    119\u001b[0m         registry \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m remote_registry\n\u001b[0;32m--> 121\u001b[0m \u001b[39mreturn\u001b[39;00m build_pipeline_graph(\n\u001b[1;32m    122\u001b[0m     input_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput,\n\u001b[1;32m    123\u001b[0m     output_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput,\n\u001b[1;32m    124\u001b[0m     registry\u001b[39m=\u001b[39;49mregistry,\n\u001b[1;32m    125\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Lia-tesis/autogoal/autogoal/autogoal/kb/_algorithm.py:690\u001b[0m, in \u001b[0;36mbuild_pipeline_graph\u001b[0;34m(input_types, output_type, registry, max_list_depth)\u001b[0m\n\u001b[1;32m    688\u001b[0m     G\u001b[39m.\u001b[39mremove_nodes_from(unreachable_nodes)\n\u001b[1;32m    689\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo pipelines can be found!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    692\u001b[0m \u001b[39mreturn\u001b[39;00m PipelineSpace(G, input_types\u001b[39m=\u001b[39minput_types)\n",
      "\u001b[0;31mTypeError\u001b[0m: No pipelines can be found!"
     ]
    }
   ],
   "source": [
    "from autogoal.utils import Hour, Min\n",
    "from autogoal.ml import AutoML\n",
    "from autogoal.search import (\n",
    "    RichLogger,\n",
    "    PESearch,\n",
    ")\n",
    "\n",
    "from autogoal.kb import *\n",
    "\n",
    "from autogoal_contrib import find_classes\n",
    "from autogoal.metalearning.image_metafeatures import ImageMetafeatureExtractor\n",
    "\n",
    "\n",
    "classifier = AutoML(\n",
    "    dataset_type= ImageMetafeatureExtractor(),\n",
    "    search_algorithm= PESearch,\n",
    "    input=(Tensor4, Supervised[VectorCategorical]),\n",
    "    output=VectorCategorical,\n",
    "    cross_validation_steps=1,\n",
    "    # Since we only want to try neural networks, we restrict\n",
    "    # the contrib registry to algorithms matching with `Keras`.\n",
    "    registry=find_classes(\"Keras\"),\n",
    "    errors=\"raise\",\n",
    "    # Since image classifiers are heavy to train, let's give them a longer timeout...\n",
    "    evaluation_timeout=5 * Min,\n",
    "    search_timeout=1 * Hour,\n",
    ")\n",
    "y = np.array(vectorized_y)\n",
    "classifier.fit(X, y, logger=RichLogger())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
